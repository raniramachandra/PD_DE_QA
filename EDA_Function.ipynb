{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d197581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19b07487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads a CSV file into a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    - file_path: Path to the CSV file.\n",
    "    \n",
    "    Returns:\n",
    "    - A DataFrame containing the data from the CSV file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"File '{file_path}' loaded successfully.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading file '{file_path}': {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage:\n",
    "# file_path = 'path_to_file.csv'\n",
    "# df = load_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fce90d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '03_Library Systembook.csv' loaded successfully.\n",
      "    Id                                     Books Book checkout Book Returned  \\\n",
      "0  1.0                       Catcher in the Rye   \"20/02/2023\"    25/02/2023   \n",
      "1  2.0          Lord of the rings the two towers  \"24/03/2023\"    21/03/2023   \n",
      "2  3.0  Lord of the rings the return of the kind  \"29/03/2023\"    25/03/2023   \n",
      "3  4.0                                The hobbit  \"02/04/2023\"    25/03/2023   \n",
      "4  5.0                                     Dune   \"02/04/2023\"    25/03/2023   \n",
      "\n",
      "  Days allowed to borrow  Customer ID  \n",
      "0                2 weeks          1.0  \n",
      "1                2 weeks          2.0  \n",
      "2                2 weeks          3.0  \n",
      "3                2 weeks          4.0  \n",
      "4                2 weeks          5.0  \n"
     ]
    }
   ],
   "source": [
    "file_path = '03_Library Systembook.csv'\n",
    "df = load_csv(file_path)\n",
    "\n",
    "if df is not None:\n",
    "    print(df.head())  # Display the first few rows of the loaded CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9ddeb25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '03_Library SystemCustomers.csv' loaded successfully.\n",
      "   Customer ID   Customer Name\n",
      "0          1.0        Jane Doe\n",
      "1          2.0      John Smith\n",
      "2          3.0      Dan Reeves\n",
      "3          NaN             NaN\n",
      "4          5.0  William Holden\n"
     ]
    }
   ],
   "source": [
    "file_path2 = '03_Library SystemCustomers.csv'\n",
    "df2 = load_csv(file_path2)\n",
    "\n",
    "if df2 is not None:\n",
    "    print(df2.head())  # Display the first few rows of the loaded CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d711cb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_na_and_log(data: pd.DataFrame, column_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drops rows with NaN values in a specific column, logs the number of rows dropped and the total number of records, \n",
    "    and returns a log DataFrame for auditing.\n",
    "    \n",
    "    Args:\n",
    "    - data: The DataFrame to clean.\n",
    "    - column_name: The column name to check for NaN values.\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple containing the cleaned DataFrame and a DataFrame containing the audit log of dropped records.\n",
    "    \"\"\"\n",
    "    # Record the number of rows before dropping NaN\n",
    "    initial_row_count = len(data)\n",
    "    \n",
    "    # Drop rows where the specified column has NaN values\n",
    "    data_cleaned = data.dropna(subset=[column_name])\n",
    "    \n",
    "    # Record the number of rows after dropping NaN\n",
    "    final_row_count = len(data_cleaned)\n",
    "    \n",
    "    # Log the number of records dropped and total records before and after\n",
    "    dropped_records = initial_row_count - final_row_count\n",
    "    log_data = {\n",
    "        'Column Name': [column_name],\n",
    "        'Records Before Drop': [initial_row_count],\n",
    "        'Records Dropped': [dropped_records],\n",
    "        'Records After Drop': [final_row_count]\n",
    "    }\n",
    "    \n",
    "    # Create the log DataFrame\n",
    "    log_df = pd.DataFrame(log_data)\n",
    "    \n",
    "    return data_cleaned, log_df\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'Book checkout' is the column where you want to drop NaN values\n",
    "# cleaned_data, audit_log = drop_na_and_log(cleaned_data, 'Book checkout')\n",
    "\n",
    "# Print cleaned data and audit log\n",
    "# print(cleaned_data)\n",
    "# print(audit_log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56fb775a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Customer ID     Customer Name\n",
      "0          1.0          Jane Doe\n",
      "1          2.0        John Smith\n",
      "2          3.0        Dan Reeves\n",
      "4          5.0    William Holden\n",
      "5          6.0     Jaztyn Forest\n",
      "6          7.0     Jackie Irving\n",
      "7          8.0  Matthew Stirling\n",
      "8          9.0         Emory Ted\n",
      "   Column Name  Records Before Drop  Records Dropped  Records After Drop\n",
      "0  Customer ID                    9                1                   8\n"
     ]
    }
   ],
   "source": [
    "# Applying the function\n",
    "cleaned_data_customer, audit_log_customer = drop_na_and_log(df2, 'Customer ID')\n",
    "\n",
    "# Print cleaned data and audit log\n",
    "print(cleaned_data_customer)\n",
    "print(audit_log_customer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aef4151f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Id                                     Books Book checkout  \\\n",
      "0    1.0                       Catcher in the Rye     2023-02-20   \n",
      "1    2.0          Lord of the rings the two towers    2023-03-24   \n",
      "2    3.0  Lord of the rings the return of the kind    2023-03-29   \n",
      "3    4.0                                The hobbit    2023-04-02   \n",
      "4    5.0                                     Dune     2023-04-02   \n",
      "5    6.0                              Little Women    2023-04-02   \n",
      "6    7.0                                        IT    2063-04-10   \n",
      "7    8.0                                   Misery     2023-04-15   \n",
      "8    9.0                                  Catch 22    2023-04-15   \n",
      "9   10.0                              Animal Farm     2023-04-20   \n",
      "10  11.0                                      1984    2023-04-23   \n",
      "11  12.0                              Little Women    2023-04-02   \n",
      "12  13.0                              East of Eden    2023-04-30   \n",
      "13  14.0                   America Is in the Heart    2023-05-01   \n",
      "14  15.0                         Wuthering Heights    2023-05-01   \n",
      "15  16.0                                Dark Tales    2023-05-15   \n",
      "17  18.0                            Les Miserables    2023-06-03   \n",
      "18  19.0                                   Dracula    2023-06-10   \n",
      "19  20.0                              Frankenstein    2023-06-01   \n",
      "\n",
      "   Book Returned Days allowed to borrow  Customer ID  \n",
      "0     2023-02-25                2 weeks          1.0  \n",
      "1     2023-03-21                2 weeks          2.0  \n",
      "2     2023-03-25                2 weeks          3.0  \n",
      "3     2023-03-25                2 weeks          4.0  \n",
      "4     2023-03-25                2 weeks          5.0  \n",
      "5     2023-05-01                2 weeks          1.0  \n",
      "6     2023-04-03                2 weeks          6.0  \n",
      "7     2023-04-03                2 weeks          7.0  \n",
      "8     2023-04-16                2 weeks          7.0  \n",
      "9     2023-04-24                2 weeks          2.0  \n",
      "10    2023-04-27                2 weeks          8.0  \n",
      "11    2023-05-01                2 weeks          1.0  \n",
      "12    2023-05-05                2 weeks          2.0  \n",
      "13    2023-05-07                2 weeks          3.0  \n",
      "14    2023-05-10                2 weeks          9.0  \n",
      "15    2023-06-01                2 weeks          2.0  \n",
      "17    2023-06-07                2 weeks          5.0  \n",
      "18    2023-07-10                2 weeks         10.0  \n",
      "19    2023-06-20                2 weeks          2.0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2696\\1138206385.py:23: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  data[column] = pd.to_datetime(data[column], errors='coerce')  # Use 'coerce' to handle invalid formats\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2696\\1138206385.py:23: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  data[column] = pd.to_datetime(data[column], errors='coerce')  # Use 'coerce' to handle invalid formats\n"
     ]
    }
   ],
   "source": [
    "def clean_and_format_data(data: pd.DataFrame, date_columns: list, string_columns: list, drop_na: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans and formats the dataset by:\n",
    "    - Stripping unwanted characters from string columns.\n",
    "    - Converting specified columns to datetime format.\n",
    "    - Optionally dropping rows with NaN values.\n",
    "\n",
    "    Args:\n",
    "    - data: The DataFrame to clean.\n",
    "    - date_columns: List of column names to convert to datetime.\n",
    "    - string_columns: List of column names to clean (remove unwanted characters).\n",
    "    - drop_na: Whether to drop rows with NaN values (default is True).\n",
    "\n",
    "    Returns:\n",
    "    - A cleaned and formatted DataFrame.\n",
    "    \"\"\"\n",
    "    # Clean string columns by removing unwanted characters (like quotes)\n",
    "    for column in string_columns:\n",
    "        data[column] = data[column].str.replace('\"', \"\", regex=True)\n",
    "    \n",
    "    # Convert specified columns to datetime format\n",
    "    for column in date_columns:\n",
    "        data[column] = pd.to_datetime(data[column], errors='coerce')  # Use 'coerce' to handle invalid formats\n",
    "\n",
    "    # Drop rows with NaN values if specified\n",
    "    if drop_na:\n",
    "        data = data.dropna()\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'Book checkout' and 'Book Returned' are the columns to clean and format\n",
    "# data = pd.DataFrame({\n",
    "#     'Book checkout': ['\"2023-01-01\"', '\"2023-02-01\"', None],\n",
    "#     'Book Returned': ['\"2023-01-10\"', '\"2023-02-15\"', '\"Invalid Date\"']\n",
    "# })\n",
    "\n",
    "# Clean and format the data\n",
    "cleaned_data = clean_and_format_data(df, date_columns=['Book checkout', 'Book Returned'], string_columns=['Book checkout', 'Book Returned'])\n",
    "\n",
    "print(cleaned_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "87e5ddc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_day_diff(data: pd.DataFrame, checkout_column: str, return_column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the number of days between two date columns and adds a new column 'day_diff'.\n",
    "    \n",
    "    Args:\n",
    "    - data: The DataFrame containing the date columns.\n",
    "    - checkout_column: The name of the 'Book checkout' column.\n",
    "    - return_column: The name of the 'Book Returned' column.\n",
    "    \n",
    "    Returns:\n",
    "    - The DataFrame with a new column 'day_diff' showing the number of days between the two dates.\n",
    "    \"\"\"\n",
    "    # Calculate the difference in days between the two date columns\n",
    "    data['day_diff'] = (data[return_column] - data[checkout_column]).dt.days\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ba1fa9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Id                                     Books Book checkout  \\\n",
      "0    1.0                       Catcher in the Rye     2023-02-20   \n",
      "1    2.0          Lord of the rings the two towers    2023-03-24   \n",
      "2    3.0  Lord of the rings the return of the kind    2023-03-29   \n",
      "3    4.0                                The hobbit    2023-04-02   \n",
      "4    5.0                                     Dune     2023-04-02   \n",
      "5    6.0                              Little Women    2023-04-02   \n",
      "6    7.0                                        IT    2063-04-10   \n",
      "7    8.0                                   Misery     2023-04-15   \n",
      "8    9.0                                  Catch 22    2023-04-15   \n",
      "9   10.0                              Animal Farm     2023-04-20   \n",
      "10  11.0                                      1984    2023-04-23   \n",
      "11  12.0                              Little Women    2023-04-02   \n",
      "12  13.0                              East of Eden    2023-04-30   \n",
      "13  14.0                   America Is in the Heart    2023-05-01   \n",
      "14  15.0                         Wuthering Heights    2023-05-01   \n",
      "15  16.0                                Dark Tales    2023-05-15   \n",
      "17  18.0                            Les Miserables    2023-06-03   \n",
      "18  19.0                                   Dracula    2023-06-10   \n",
      "19  20.0                              Frankenstein    2023-06-01   \n",
      "\n",
      "   Book Returned Days allowed to borrow  Customer ID  day_diff  \n",
      "0     2023-02-25                2 weeks          1.0         5  \n",
      "1     2023-03-21                2 weeks          2.0        -3  \n",
      "2     2023-03-25                2 weeks          3.0        -4  \n",
      "3     2023-03-25                2 weeks          4.0        -8  \n",
      "4     2023-03-25                2 weeks          5.0        -8  \n",
      "5     2023-05-01                2 weeks          1.0        29  \n",
      "6     2023-04-03                2 weeks          6.0    -14617  \n",
      "7     2023-04-03                2 weeks          7.0       -12  \n",
      "8     2023-04-16                2 weeks          7.0         1  \n",
      "9     2023-04-24                2 weeks          2.0         4  \n",
      "10    2023-04-27                2 weeks          8.0         4  \n",
      "11    2023-05-01                2 weeks          1.0        29  \n",
      "12    2023-05-05                2 weeks          2.0         5  \n",
      "13    2023-05-07                2 weeks          3.0         6  \n",
      "14    2023-05-10                2 weeks          9.0         9  \n",
      "15    2023-06-01                2 weeks          2.0        17  \n",
      "17    2023-06-07                2 weeks          5.0         4  \n",
      "18    2023-07-10                2 weeks         10.0        30  \n",
      "19    2023-06-20                2 weeks          2.0        19  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_2696\\174341260.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['day_diff'] = (data[return_column] - data[checkout_column]).dt.days\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'Book checkout' and 'Book Returned' are the columns in the DataFrame\n",
    "cleaned_data = calculate_day_diff(cleaned_data, 'Book checkout', 'Book Returned')\n",
    "\n",
    "print(cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cccd1b7",
   "metadata": {},
   "source": [
    "LOADING SQL server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d483c688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: sqlalchemy in c:\\programdata\\anaconda3\\lib\\site-packages (2.0.25)\n",
      "Requirement already satisfied: pyodbc in c:\\programdata\\anaconda3\\lib\\site-packages (5.0.1)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sqlalchemy) (4.9.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from sqlalchemy) (3.0.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "#!pip install pandas sqlalchemy pyodbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40fb2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ODBC Drivers available:\n",
      "SQL Server\n",
      "SQL Server Native Client RDA 11.0\n",
      "ODBC Driver 17 for SQL Server\n",
      "Microsoft Access Driver (*.mdb, *.accdb)\n",
      "Microsoft Excel Driver (*.xls, *.xlsx, *.xlsm, *.xlsb)\n",
      "Microsoft Access Text Driver (*.txt, *.csv)\n"
     ]
    }
   ],
   "source": [
    "# Checking the ODBC Driver for SQL Server\n",
    "# import pyodbc\n",
    "\n",
    "# # List all ODBC drivers installed on the system\n",
    "# drivers = [driver for driver in pyodbc.drivers()]\n",
    "# print(\"ODBC Drivers available:\")\n",
    "# for driver in drivers:\n",
    "#     print(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246d06e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import create_engine\n",
    "\n",
    "# # Define the connection string to your MS SQL Server\n",
    "# server = 'localhost'  \n",
    "# database = 'QAETLStagingDB'\n",
    "# username = 'python_app'\n",
    "# password = 'password'\n",
    "\n",
    "# # Create the connection string with Windows Authentication\n",
    "# connection_string = f'mssql+pyodbc://@{server}/{database}?trusted_connection=yes&driver=ODBC+Driver+17+for+SQL+Server'\n",
    "\n",
    "\n",
    "# # Create the SQLAlchemy engine\n",
    "# engine = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd0d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Write the DataFrame to SQL Server\n",
    "# cleaned_data.to_sql('book_library', con=engine, if_exists='replace', index=False)\n",
    "# cleaned_data_customer.to_sql('customer_library', con=engine, if_exists='replace', index=False)\n",
    "# audit_log_customer.to_sql('audit_log_customer', con=engine, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ec44d",
   "metadata": {},
   "source": [
    "CONVERTING to .PY file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d561b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open your terminal (either in VS Code or elsewhere).\n",
    "\n",
    "# Run the following command to convert your notebook to a .py file:\n",
    "\n",
    "# jupyter nbconvert --to script your_notebook.ipynb\n",
    "\n",
    "\n",
    "# This will generate a Python script (your_notebook.py) in the same folder as the .ipynb file, containing only the code cells."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
